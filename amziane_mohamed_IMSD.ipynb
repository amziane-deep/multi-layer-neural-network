{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0047178c-3f1b-4e79-803a-87afcf5c5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix ,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7e8696d-3d11-47e0-b609-f51384a5cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    assert isinstance(x, np.ndarray), \"Input to ReLU must be a numpy array\"\n",
    "    result = np.maximum(0, x)\n",
    "    assert np.all(result >= 0), \"ReLU output must be non-negative\"\n",
    "    return result\n",
    "\n",
    "def relu_derivative(x):\n",
    "    assert isinstance(x, np.ndarray), \"Input to ReLU derivative must be a numpy array\"\n",
    "    result = np.where(x > 0, 1, 0)\n",
    "    assert np.all((result == 0) | (result == 1)), \"ReLU derivative must be 0 or 1\"\n",
    "    return result\n",
    "\n",
    "def softmax(x):\n",
    "    assert isinstance(x, np.ndarray), \"Input to softmax must be a numpy array\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # pour stabilité numérique\n",
    "    result = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    assert np.all((result >= 0) & (result <= 1)), \"Softmax output must be in [0, 1]\"\n",
    "    assert np.allclose(np.sum(result, axis=1), 1), \"Softmax output must sum to 1 per sample\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f49702d9-5a2c-472c-87e1-75755736c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y_pred, Y_true, weights, lambda_):\n",
    "    loss = -np.mean(Y_true * np.log(Y_pred) + (1 - Y_true) * np.log(1 - Y_pred))\n",
    "    reg_term = 0\n",
    "    for w in weights:\n",
    "        reg_term += np.sum(w ** 2)\n",
    "    reg_term *= lambda_ / 2\n",
    "    return loss + reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7fc7cd81-8476-4d06-84a8-d1040f1a313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:218: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:218: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14620\\725336431.py:218: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print (\"\\ nRapport de classification ( Test set ) :\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Downloads\\amhcd-data-64\\tifinagh-images\\\n",
      "C:\\Users\\Admin\\Downloads\n",
      "labels-map.csv not found . Please check the dataset structure .\n",
      " Loaded 28182 samples with 33 unique classes .\n",
      " Train : 16908 samples , Validation : 5637 samples , Test : 5637 samples \n",
      "Epoch 0: Train Loss = 3.5033, Val Loss = 3.4965, Train Acc = 0.0396, Val Acc = 0.0373\n",
      "Epoch 10: Train Loss = 3.5027, Val Loss = 3.4959, Train Acc = 0.0533, Val Acc = 0.0495\n",
      "Epoch 20: Train Loss = 3.0726, Val Loss = 3.0363, Train Acc = 0.0966, Val Acc = 0.0953\n",
      "Epoch 30: Train Loss = 2.3341, Val Loss = 2.3010, Train Acc = 0.3049, Val Acc = 0.3003\n",
      "Epoch 40: Train Loss = 1.5625, Val Loss = 1.6036, Train Acc = 0.5117, Val Acc = 0.4882\n",
      "Epoch 50: Train Loss = 1.0960, Val Loss = 1.1550, Train Acc = 0.6635, Val Acc = 0.6422\n",
      "Epoch 60: Train Loss = 0.7771, Val Loss = 0.8465, Train Acc = 0.7711, Val Acc = 0.7367\n",
      "Epoch 70: Train Loss = 0.5665, Val Loss = 0.6783, Train Acc = 0.8214, Val Acc = 0.7798\n",
      "Epoch 80: Train Loss = 0.4245, Val Loss = 0.5683, Train Acc = 0.8692, Val Acc = 0.8153\n",
      "Epoch 90: Train Loss = 0.3293, Val Loss = 0.4811, Train Acc = 0.9102, Val Acc = 0.8465\n",
      "Epoch 99: Train Loss = 0.2654, Val Loss = 0.4797, Train Acc = 0.9145, Val Acc = 0.8489\n",
      "\\ nRapport de classification ( Test set ) :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ya       0.93      0.94      0.93       171\n",
      "         yab       0.80      0.78      0.79       171\n",
      "        yach       0.79      0.87      0.83       171\n",
      "         yad       0.89      0.82      0.86       171\n",
      "        yadd       0.78      0.74      0.76       171\n",
      "         yae       0.92      0.92      0.92       171\n",
      "         yaf       0.88      0.84      0.86       171\n",
      "         yag       0.89      0.89      0.89       171\n",
      "        yagg       0.88      0.94      0.91       170\n",
      "        yagh       0.88      0.94      0.91       170\n",
      "         yah       0.86      0.91      0.88       171\n",
      "        yahh       0.82      0.90      0.86       171\n",
      "         yaj       0.85      0.84      0.84       171\n",
      "         yak       0.85      0.83      0.84       171\n",
      "        yakk       0.94      0.87      0.90       171\n",
      "         yal       0.94      0.81      0.87       170\n",
      "         yam       0.83      0.88      0.86       171\n",
      "         yan       0.80      0.94      0.86       170\n",
      "         yaq       0.85      0.93      0.89       171\n",
      "         yar       0.88      0.73      0.80       171\n",
      "        yarr       0.84      0.81      0.82       171\n",
      "         yas       0.65      0.69      0.67       171\n",
      "        yass       0.90      0.82      0.86       171\n",
      "         yat       0.88      0.89      0.89       171\n",
      "        yatt       0.76      0.66      0.71       171\n",
      "         yaw       0.95      0.82      0.88       171\n",
      "         yax       0.96      0.92      0.94       170\n",
      "         yay       0.88      0.91      0.90       171\n",
      "         yaz       0.70      0.93      0.80       171\n",
      "        yazz       0.89      0.68      0.77       171\n",
      "         yey       0.74      0.85      0.79       171\n",
      "          yi       0.75      0.76      0.75       170\n",
      "          yu       0.82      0.78      0.80       171\n",
      "\n",
      "    accuracy                           0.84      5637\n",
      "   macro avg       0.85      0.84      0.84      5637\n",
      "weighted avg       0.85      0.84      0.84      5637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MultiClassNeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.1,l2_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with given layer sizes and learning rate.\n",
    "        layer_sizes: List of integers [input_size, hidden1_size,\n",
    "        ..., output_size]\n",
    "        \"\"\"\n",
    "        assert isinstance(layer_sizes,list) and len(layer_sizes) >= 2,\"layer_sizes must be a list with at least 2 elements\"\n",
    "        assert all(isinstance(size, int) and size > 0 for size in layer_sizes), \"All layer sizes must be positive integers\"\n",
    "        assert isinstance(learning_rate, (int, float)) and learning_rate > 0, \"Learning rate must be a positive number\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "         # Initialisation des poids et biais\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(layer_sizes)- 1):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            assert w.shape == (layer_sizes[i], layer_sizes[i+1]),f\"Weight matrix {i+1} has incorrect shape\"\n",
    "            assert b.shape == (1, layer_sizes[i+1]), f\"Bias vector {i+1} has incorrect shape\"\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    def forward(self, X):\n",
    "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "    \n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "    \n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(self.activations[-1] , self.weights[i]) + self.biases[i]\n",
    "            assert z.shape == (X.shape[0], self.layer_sizes[i + 1]), f\"Z^{i+1} has incorrect shape\"\n",
    "            self.z_values.append(z)\n",
    "            self.activations.append(relu(z))\n",
    "    \n",
    "        z = np.dot(self.activations[-1] , self.weights[-1]) + self.biases[-1]\n",
    "        assert z.shape == (X.shape[0], self.layer_sizes[-1]), \"Output Z has incorrect shape\"\n",
    "        self.z_values.append(z)\n",
    "        output = softmax(z)\n",
    "        assert output.shape == (X.shape[0], self.layer_sizes[-1]), \"Output A has incorrect shape\"\n",
    "        self.activations.append(output)\n",
    "    \n",
    "        return self.activations [-1]\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to loss must be numpy arrays\"\n",
    "        assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "    \n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "        l2_penalty = self.l2_lambda / (2 * y_true.shape[0]) * sum(np.sum(w ** 2) for w in self.weights)\n",
    "        assert not np.isnan(loss), \"Loss computation resulted in NaN\"\n",
    "        return loss+l2_penalty\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to accuracy must be numpy arrays\"\n",
    "        assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "    \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "        true_labels = np.argmax(y_true, axis=1)\n",
    "        accuracy = np.mean(predictions == true_labels)\n",
    "        assert 0 <= accuracy <= 1, \"Accuracy must be between 0 and 1\"\n",
    "        return accuracy\n",
    "    def backward(self, X, y, outputs):\n",
    "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray) and isinstance(outputs, np.ndarray), \"Inputs to backward must be numpy arrays\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y.shape == outputs.shape, \"y and outputs must have the same shape\"\n",
    "    \n",
    "        m = X.shape[0]\n",
    "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
    "    \n",
    "        dZ = outputs - y # Gradient pour softmax + cross - entropy\n",
    "        assert dZ. shape == outputs .shape , \"dZ for output layer has incorrect shape \"\n",
    "        self.d_weights[-1] = (self.activations[-2].T @ dZ) / m\n",
    "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "    \n",
    "        for i in range(len(self.weights) - 2, -1, -1):\n",
    "            dZ = np.dot(dZ , self.weights[i + 1].T) * relu_derivative(self.z_values[i])\n",
    "            assert dZ. shape == (X. shape [0] , self . layer_sizes [i +1]), f\"dZ ^{[i +1]} has incorrect shape \"\n",
    "            self.d_weights[i] = (np.dot(self.activations[i].T , dZ)) / m\n",
    "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        # TODO : Ajouter une r g u l a r i s a t i o n L2 aux gradients des poids\n",
    "        # dW ^{[ l]} += lambda * W ^{[ l]} / m, o lambda est le coefficient de r g u l a r i s a t i o n\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * self.d_weights[i]\n",
    "            self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
    "    def train(self, X, y, X_val, y_val, epochs, batch_size):\n",
    "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray), \"X and y must be numpy arrays\"\n",
    "        assert isinstance(X_val, np.ndarray) and isinstance(y_val, np.ndarray), \"X_val and y_val must be numpy arrays\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y.shape[1] == self.layer_sizes[-1], f\"Output dimension ({y.shape[1]}) must match output layer size ({self.layer_sizes[-1]})\"\n",
    "        assert X_val.shape[1] == self.layer_sizes[0], f\"Validation input dimension ({X_val.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y_val.shape[1] == self.layer_sizes[-1], f\"Validation output dimension ({y_val.shape[1]}) must match output layer size ({self.layer_sizes[-1]})\"\n",
    "        assert isinstance(epochs, int) and epochs > 0, \"Epochs must be a positive integer\"\n",
    "        assert isinstance(batch_size, int) and batch_size > 0, \"Batch size must be a positive integer\"\n",
    "    \n",
    "        train_losses, val_losses = [],[]\n",
    "        train_accuracies, val_accuracies = [],[]\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "    \n",
    "            epoch_loss = 0\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "    \n",
    "                outputs = self.forward(X_batch)\n",
    "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
    "                self.backward(X_batch, y_batch, outputs)\n",
    "   #============================================================================================== \n",
    "            # Calculer les pertes et accuracies\n",
    "            train_loss = epoch_loss / (X.shape[0] // batch_size)\n",
    "            train_pred = self.forward(X)\n",
    "            train_acc = self.compute_accuracy(y, train_pred)\n",
    "            val_pred = self.forward(X_val)\n",
    "            val_loss = self.compute_loss(y_val, val_pred)\n",
    "            val_acc = self.compute_accuracy(y_val, val_pred)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_accuracies.append(val_acc)\n",
    "    \n",
    "            if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "                print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "    def predict(self, X):\n",
    "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "    \n",
    "        outputs = self.forward(X)\n",
    "        predictions = np.argmax(outputs, axis=1)\n",
    "        assert predictions.shape == (X.shape[0],), \"Predictions have incorrect shape\"\n",
    "        return predictions\n",
    " #D f i n i r le chemin vers le dossier d c o m p r e s s\n",
    "data_dir = os.path .join ( os . getcwd () , 'C:\\\\Users\\\\Admin\\\\Downloads\\\\amhcd-data-64\\\\tifinagh-images\\\\')\n",
    "print ( data_dir )\n",
    "current_working_directory = os . getcwd ()\n",
    "print ( current_working_directory )\n",
    "# Charger le fichier CSV contenant les tiquettes\n",
    "try:\n",
    "    labels_df = pd.read_csv ( os.path.join ( data_dir , 'amhcd-data-64/labels-map .csv ') )\n",
    "    assert 'image_path ' in labels_df . columns and 'label ' in labels_df.columns , \"CSV must contain ’ image_path ’ and ’ label ’ columns \"\n",
    "except FileNotFoundError :\n",
    "    print (\"labels-map.csv not found . Please check the dataset structure .\")\n",
    "    # Alternative : construire un DataFrame partir des dossiers\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label_dir in os.listdir ( data_dir ):\n",
    "        label_path = os.path . join ( data_dir , label_dir )\n",
    "        if os . path.isdir ( label_path ) :\n",
    "            for img_name in os.listdir ( label_path ) :\n",
    "                image_paths . append ( os.path.join ( label_path ,img_name ))\n",
    "                labels.append ( label_dir )\n",
    "    labels_df = pd.DataFrame ({ 'image_path ': image_paths , 'label ':labels })\n",
    "# V r i f i e r le DataFrame\n",
    "assert not labels_df.empty , \"No data loaded . Check dataset files .\"\n",
    "print (f\" Loaded {len ( labels_df )} samples with { labels_df [ 'label '].nunique ()} unique classes .\")\n",
    "\n",
    "# Encoder les tiquettes\n",
    "label_encoder = LabelEncoder ()\n",
    "labels_df ['label_encoded'] = label_encoder . fit_transform ( labels_df['label '])\n",
    "num_classes = len ( label_encoder . classes_ )\n",
    "# Fonction pour charger et p r t r a i t e r une image\n",
    "def load_and_preprocess_image ( image_path , target_size =(32 , 32) ):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image : convert to grayscale , resize normalize\n",
    "    \"\"\"\n",
    "    assert os.path.exists ( image_path ) , f\" Image not found :{ image_path }\"\n",
    "    img = cv2 . imread ( image_path , cv2 . IMREAD_GRAYSCALE )\n",
    "    assert img is not None , f\" Failed to load image : { image_path }\"\n",
    "    img = cv2.resize ( img , target_size )\n",
    "    img = img.astype ( np . float32 ) / 255.0 # Normalisation\n",
    "    return img.flatten () # Aplatir pour le r s e a u de neurones\n",
    "# Charger toutes les images\n",
    "X = np.array ([ load_and_preprocess_image ( os.path.join ( data_dir ,path )) for path in labels_df ['image_path ']])\n",
    "y = labels_df ['label_encoded'].values\n",
    "# V r i f i e r les dimensions\n",
    "assert X.shape [0] == y.shape [0] , \" Mismatch between number of images and labels \"\n",
    "assert X.shape [1] == 32 * 32 , f\" Expected flattened image size of {32*32} , got {X.shape [1]} \"\n",
    "\n",
    "# Diviser en ensembles d’ e n t r a n e m e n t , validation et test\n",
    "X_temp , X_test , y_temp , y_test = train_test_split (X , y , test_size =0.2 , stratify =y , random_state =42)\n",
    "X_train , X_val , y_train , y_val = train_test_split ( X_temp , y_temp ,test_size =0.25 , stratify = y_temp , random_state =42)\n",
    "\n",
    "# Convertir explicitement en NumPy arrays\n",
    "X_train = np.array ( X_train )\n",
    "X_val = np.array ( X_val )\n",
    "X_test = np.array ( X_test )\n",
    "y_train = np.array ( y_train )\n",
    "y_val = np.array ( y_val )\n",
    "y_test = np.array ( y_test )\n",
    "assert X_train .shape [0] + X_val.shape [0] + X_test.shape [0] == X.shape [0] , \"Train -val - test split sizes must sum to total samples\"\n",
    "print (f\" Train : { X_train.shape [0]} samples , Validation : { X_val.shape [0]} samples , Test : { X_test.shape [0]} samples \")\n",
    "# Encoder les tiquettes en one - hot pour la classification multiclasse\n",
    "one_hot_encoder = OneHotEncoder ( sparse_output = False )\n",
    "y_train_one_hot = np.array ( one_hot_encoder . fit_transform ( y_train.reshape ( -1 , 1) ))\n",
    "y_val_one_hot = np.array ( one_hot_encoder.transform ( y_val.reshape( -1 , 1) ))\n",
    "y_test_one_hot = np.array ( one_hot_encoder.transform ( y_test.reshape( -1 , 1) ))\n",
    "\n",
    "# Verifier que les tableaux one - hot sont des NumPy arrays\n",
    "assert isinstance ( y_train_one_hot , np.ndarray ) , \" y_train_one_hot must be a numpy array \"\n",
    "assert isinstance ( y_val_one_hot , np.ndarray ) , \" y_val_one_hot must be a numpy array \"\n",
    "assert isinstance ( y_test_one_hot , np.ndarray ) , \" y_test_one_hot must be a numpy array \"\n",
    "\n",
    "# Creer et en treaner le modele\n",
    "layer_sizes = [ X_train.shape [1] , 64 , 32 , num_classes ] # 64 et 32 neurones c a c h s , 33 classes\n",
    "nn = MultiClassNeuralNetwork ( layer_sizes , learning_rate =0.01)\n",
    "train_losses , val_losses , train_accuracies , val_accuracies = nn.train (X_train , y_train_one_hot , X_val , y_val_one_hot , epochs =100 ,batch_size =32)\n",
    "\n",
    "\n",
    "# Predictions et valuation\n",
    "y_pred = nn.predict ( X_test )\n",
    "print (\"\\ nRapport de classification ( Test set ) :\")\n",
    "print ( classification_report ( y_test , y_pred , target_names =label_encoder . classes_ ))\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix ( y_test , y_pred )\n",
    "plt.figure ( figsize =(10 , 8) )\n",
    "sns.heatmap (cm , annot = True , fmt ='d', cmap ='Blues')\n",
    "plt.title ('Matrice de confusion ( Test set)')\n",
    "plt.xlabel ('Predit')\n",
    "plt.ylabel ('Reel')\n",
    "plt.savefig ('confusion_matrix.png')\n",
    "plt.close ()\n",
    "\n",
    "# Courbes de perte et d’accuracy\n",
    "fig , ( ax1 , ax2 ) = plt.subplots (1 , 2, figsize =(12, 5) )\n",
    "\n",
    "# Courbe de perte\n",
    "ax1.plot ( train_losses , label ='Train Loss')\n",
    "ax1.plot ( val_losses , label ='Validation Loss')\n",
    "ax1.set_title ('Courbe de perte ')\n",
    "ax1.set_xlabel ('poque')\n",
    "ax1.set_ylabel ('Perte')\n",
    "ax1.legend ()\n",
    "\n",
    "# Courbe d’accuracy\n",
    "ax2.plot ( train_accuracies , label ='Train Accuracy')\n",
    "ax2.plot ( val_accuracies , label ='Validation Accuracy')\n",
    "ax2.set_title ('Courbe de precision')\n",
    "ax2.set_xlabel ('poque')\n",
    "ax2.set_ylabel ('Prcision')\n",
    "ax2.legend ()\n",
    "plt.tight_layout ()\n",
    "fig.savefig ('loss_accuracy_plot.png')\n",
    "plt.close ()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7f10619-913e-4877-a64d-ecb943564c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 3.5041, Val Loss = 3.4962, Train Acc = 0.0303, Val Acc = 0.0303\n",
      "Epoch 10: Train Loss = 0.4027, Val Loss = 0.5970, Train Acc = 0.8530, Val Acc = 0.8088\n",
      "Epoch 20: Train Loss = 0.1244, Val Loss = 0.3238, Train Acc = 0.9593, Val Acc = 0.8975\n",
      "Epoch 30: Train Loss = 0.0694, Val Loss = 0.3308, Train Acc = 0.9791, Val Acc = 0.9065\n",
      "Epoch 40: Train Loss = 0.0315, Val Loss = 0.3529, Train Acc = 0.9830, Val Acc = 0.9124\n",
      "Epoch 50: Train Loss = 0.0044, Val Loss = 0.2807, Train Acc = 0.9998, Val Acc = 0.9297\n",
      "Epoch 60: Train Loss = 0.0017, Val Loss = 0.2819, Train Acc = 0.9999, Val Acc = 0.9310\n",
      "Epoch 70: Train Loss = 0.0013, Val Loss = 0.2917, Train Acc = 0.9999, Val Acc = 0.9317\n",
      "Epoch 80: Train Loss = 0.0008, Val Loss = 0.2978, Train Acc = 1.0000, Val Acc = 0.9331\n",
      "Epoch 90: Train Loss = 0.0007, Val Loss = 0.3034, Train Acc = 1.0000, Val Acc = 0.9337\n",
      "Epoch 99: Train Loss = 0.0010, Val Loss = 0.3078, Train Acc = 1.0000, Val Acc = 0.9337\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [X_train.shape [1] , 64 , 32 , num_classes ] # 64 et 32 neurones c a c h s , 33 classes\n",
    "nn = MultiClassNeuralNetwork ( layer_sizes , learning_rate =0.1)\n",
    "train_losses , val_losses , train_accuracies , val_accuracies = nn .train (X_train , y_train_one_hot , X_val , y_val_one_hot , epochs =100 ,batch_size =32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4882c11-d493-4bd1-af91-8cffd82f1d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
